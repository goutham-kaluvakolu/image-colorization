# -*- coding: utf-8 -*-
"""Copy of Copy of Image Colorization With GANs.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DJy9lPoxA06jHQi4z4m02w_O8jk1c0MI

# **Image Colorization With GANs**

GANs are the state-of-the-art machine learning models which can generate new data instances from existing ones. They use a very interesting technique, inspired from the Game Theory, to generate realistic samples.

In this notebook, we'll use GANs to colorize a grayscale ( B/W ) image. In addition to that, our generator model will have a structure similar to that of a UNet i.e the one with skip connections.

## **1. Downloading and Processing the data**

A dataset of RGB images to train the GAN model whose images consists of various scenes/places.

* Download the dataset on your machine from [here](https://drive.google.com/file/d/1sQ5C8HiKVr2Edp3ojLLNauwRbOLfVn2q/view?usp=sharing).
* Upload the downloaded `.zip` file here on Colab.
"""

from google.colab import drive
drive.mount('/content/drive')

!nvidia-smi

from PIL import Image
from sklearn.model_selection import train_test_split
import tensorflow as tf
import numpy as np
from tensorflow.keras.utils import plot_model
from matplotlib import image
from matplotlib import pyplot as plt
import os
import time
import tensorflow as tf
from tensorflow import keras
tf.config.run_functions_eagerly(True)
import os
import cv2
from sklearn.model_selection import train_test_split
temp = tf.test.gpu_device_name()
print(temp)
print(tf.test.is_gpu_available())

!unzip /content/drive/MyDrive/cv_dat/Y.zip
!unzip /content/drive/MyDrive/cv_dat/X.zip

"""
We'll now parse the images ( RGB images to be precise ) one by one, and transform each one to a grayscale image using PIL's `.convert( 'L' )` method. So our dataset will have samples of $( \ grayscale \ image \ , \ RGB \ image \ )$

We used only a part of our dataset, determined by `dataset_split` , as Colab's computational power would cease on providing a large number of images.
"""

batch_size = 16

def divide_image(image):
    height, width = image.shape[:2]

    # if height != or width != 1400:
    #     raise ValueError("Image size must be 900 x 1400.")

    part_height = height // 2
    part_width = width // 4

    divided_parts = []

    for i in range(2):
        for j in range(4):
            y_start = i * part_height
            y_end = (i + 1) * part_height
            x_start = j * part_width
            x_end = (j + 1) * part_width

            part = image[y_start:y_end, x_start:x_end]
            divided_parts.append(part)

    return divided_parts



crop_width = 120
crop_height = 120
# The batch size we'll use for training

# Size of the image required to train our model
img_size = 120
x = 400
y = 900
width = 240
height = 260

# These many images will be used from the data archive
dataset_split = 400
X_dir = '/content/X'
y_dir = '/content/Y/Y'
x = []
y = []
for image_file in os.listdir(y_dir)[:dataset_split]:
    rgb_image = cv2.imread(os.path.join(y_dir, image_file))
    # rgb_image = cv2.cvtColor(rgb_image, cv2.COLOR_BGR2GRAY)
    # rgb_image = rgb_image / 255.0
    divided_parts = divide_image(rgb_image)

# Display or further process the divided parts as needed
    for i, part in enumerate(divided_parts):
        # cv2.imshow(f"Part {i}", part)
        rgb_image_c1 = cv2.resize(part, (img_size, img_size))
        rgb_image_c1 = cv2.cvtColor(rgb_image_c1, cv2.COLOR_BGR2GRAY)
        gray_img_array_c1 = rgb_image_c1.reshape((img_size, img_size, 1)) / 255.0
        x.append(gray_img_array_c1)






    # c=rgb_image[:,:958,:]
    # # print(c.shape)

    # c1=c[:700,:479,:]
    # c2=c[700:,479:,:]
    # c3=c[:700,479:,:]
    # c4=c[700:,:479,:]

    # # cropped_image = crop_image_top_middle(rgb_image, crop_width, crop_height)
    # rgb_image_c1 = cv2.resize(c1, (img_size, img_size))
    # rgb_image_c2 = cv2.resize(c2, (img_size, img_size))
    # rgb_image_c3 = cv2.resize(c3, (img_size, img_size))
    # rgb_image_c4 = cv2.resize(c4, (img_size, img_size))
    # # print(rgb_image_c1.shape)


    # rgb_image_c1 = cv2.cvtColor(rgb_image_c1, cv2.COLOR_BGR2GRAY)
    # rgb_image_c2 = cv2.cvtColor(rgb_image_c2, cv2.COLOR_BGR2GRAY)

    # rgb_image_c3 = cv2.cvtColor(rgb_image_c3, cv2.COLOR_BGR2GRAY)
    # rgb_image_c4 = cv2.cvtColor(rgb_image_c4, cv2.COLOR_BGR2GRAY)


    # # Normalize the RGB image array
    # # rgb_img_array = cropped_image / 255.0
    # # Normalize the grayscale image array
    # gray_img_array_c1 = rgb_image_c1.reshape((img_size, img_size, 1)) / 255.0
    # gray_img_array_c2 = rgb_image_c2.reshape((img_size, img_size, 1)) / 255.0

    # gray_img_array_c3 = rgb_image_c3.reshape((img_size, img_size, 1)) / 255.0
    # gray_img_array_c4 = rgb_image_c4.reshape((img_size, img_size, 1)) / 255.0


    # # Append both the image arrays
    # x.append(gray_img_array_c1)
    # x.append(gray_img_array_c2)
    # x.append(gray_img_array_c3)
    # x.append(gray_img_array_c4)



    # o_rgb_image = cv2.imread(os.path.join(y_dir, image_file))
    # o_rgb_image = cv2.cvtColor(o_rgb_image, cv2.COLOR_BGRA2RGB)

    # c=o_rgb_image[:,:958,:]
    # # print(c.shape)

    # c1=c[:700,:479,:]
    # c2=c[700:,479:,:]
    # c3=c[:700,479:,:]
    # c4=c[700:,:479,:]

    # o_rgb_image_c1 = cv2.resize(c1, (img_size, img_size))
    # o_rgb_image_c2 = cv2.resize(c2, (img_size, img_size))
    # o_rgb_image_c3 = cv2.resize(c3, (img_size, img_size))
    # o_rgb_image_c4 = cv2.resize(c4, (img_size, img_size))

    # y.append(o_rgb_image_c1)
    # y.append(o_rgb_image_c2)
    # y.append(o_rgb_image_c3)
    # y.append(o_rgb_image_c4)


# print("after x loop", len(x))
# print("x,y len 1", x[0].shape, y[0].shape)
np.save("/content/drive/MyDrive/cv_dat/X.npy",x)
x=[]



# plt.imshow(rgb_image)
# plt.show()
# # Train-test splitting
# train_x, test_x, train_y, test_y = train_test_split(np.array(x), np.array(y), test_size=0.1)
# print(train_x.shape, test_x.shape, train_y.shape, test_y.shape)
# # Construct tf.data.Dataset object
# dataset = tf.data.Dataset.from_tensor_slices( ( train_x , train_y ) )
# dataset = dataset.batch( batch_size )

y=[]
for image_file in os.listdir(y_dir)[:dataset_split]:
    rgb_image = cv2.imread(os.path.join(y_dir, image_file))
    o_rgb_image = cv2.cvtColor(rgb_image, cv2.COLOR_BGRA2RGB)

    # rgb_image = cv2.cvtColor(rgb_image, cv2.COLOR_BGR2GRAY)
    # rgb_image = rgb_image / 255.0
    divided_parts = divide_image(o_rgb_image)

# Display or further process the divided parts as needed
    for i, part in enumerate(divided_parts):
        rgb_image_c1 = cv2.resize(part, (img_size, img_size))
        # rgb_image_c1 = cv2.cvtColor(rgb_image_c1, cv2.COLOR_BGR2GRAY)
        rgb_image_c1 = rgb_image_c1 / 255.0
        y.append(rgb_image_c1)


print(len(y))
print(y[0].shape)
# for image_file in os.listdir(y_dir)[:dataset_split]:

#     o_rgb_image = cv2.imread(os.path.join(y_dir, image_file))
#     o_rgb_image = cv2.cvtColor(o_rgb_image, cv2.COLOR_BGRA2RGB)

#     c=o_rgb_image[:,:958,:]
#     # print(c.shape)

#     c1=c[:700,:479,:]
#     c2=c[700:,479:,:]
#     c3=c[:700,479:,:]
#     c4=c[700:,:479,:]

#     o_rgb_image_c1 = cv2.resize(c1, (img_size, img_size))
#     o_rgb_image_c2 = cv2.resize(c2, (img_size, img_size))
#     o_rgb_image_c3 = cv2.resize(c3, (img_size, img_size))
#     o_rgb_image_c4 = cv2.resize(c4, (img_size, img_size))

#     y.append(o_rgb_image_c1)
#     y.append(o_rgb_image_c2)
#     y.append(o_rgb_image_c3)
#     y.append(o_rgb_image_c4)


# # print("after x loop", len(x))
# # print("x,y len 1", x[0].shape, y[0].shape)
np.save("/content/drive/MyDrive/cv_dat/Y.npy",y)
y=[]

drive_dir = '/content/drive/MyDrive/cv_dat/'
os.chdir(drive_dir)
y = np.load('Y.npy')
x=np.load('X.npy')
train_x, test_x, train_y, test_y = train_test_split(np.array(x), np.array(y), test_size=0.1)
print(train_x.shape, test_x.shape, train_y.shape, test_y.shape)
# tf.compat.v1.disable_eager_execution()
# Construct tf.data.Dataset object
dataset = tf.data.Dataset.from_tensor_slices( ( train_x , train_y ) )
batch_size=16
dataset = dataset.batch( batch_size )
# tf.config.experimental_run_functions_eagerly(True)

print(train_x.shape)
print(train_y[0].shape)
print(test_x[0].shape)
plt.subplot(2,2,1)
plt.imshow(train_x[0],cmap="gray")
plt.subplot(2,2,2)
plt.imshow(train_y[0])

"""## **2. The GAN**

In this section, we'll create our GAN model step-by-step with Keras. First, we'll implement the generator then the discriminator and finally the loss functions required by both of them.

### **A. Generator**

Our generator ( represented as $G$ ) will take in grayscale image $x$ and produce a RGB image $G( x )$. Note, $x$ will be a tensor of shape $( \ batch \ size \ , \ 120 \ , \ 120 \ , \ 1 \ )$ and the output $G(x)$ will have a shape $( \ batch \ size \ , \ 120 \ , \ 120 \ , \ 3 \ )$

* Our generator will have a encoder-decoder structure, similar to the UNet architecture. Additionally, we use Dilated convolutions to have a larger receptive field.

* We introduce skip connections in our model so as to have better flow of information from the encoder to the decoder.
"""

img_size = 120

def get_generator_model():

    inputs = tf.keras.layers.Input( shape=( img_size , img_size , 1 ) )

    conv1 = tf.keras.layers.Conv2D( 16 , kernel_size=( 5 , 5 ) , strides=1 )( inputs )
    conv1 = tf.keras.layers.LeakyReLU()( conv1 )
    conv1 = tf.keras.layers.Conv2D( 32 , kernel_size=( 3 , 3 ) , strides=1)( conv1 )
    conv1 = tf.keras.layers.LeakyReLU()( conv1 )
    conv1 = tf.keras.layers.Conv2D( 32 , kernel_size=( 3 , 3 ) , strides=1)( conv1 )
    conv1 = tf.keras.layers.LeakyReLU()( conv1 )

    conv2 = tf.keras.layers.Conv2D( 32 , kernel_size=( 5 , 5 ) , strides=1)( conv1 )
    conv2 = tf.keras.layers.LeakyReLU()( conv2 )
    conv2 = tf.keras.layers.Conv2D( 64 , kernel_size=( 3 , 3 ) , strides=1 )( conv2 )
    conv2 = tf.keras.layers.LeakyReLU()( conv2 )
    conv2 = tf.keras.layers.Conv2D( 64 , kernel_size=( 3 , 3 ) , strides=1 )( conv2 )
    conv2 = tf.keras.layers.LeakyReLU()( conv2 )

    conv3 = tf.keras.layers.Conv2D( 64 , kernel_size=( 5 , 5 ) , strides=1 )( conv2 )
    conv3 = tf.keras.layers.LeakyReLU()( conv3 )
    conv3 = tf.keras.layers.Conv2D( 128 , kernel_size=( 3 , 3 ) , strides=1 )( conv3 )
    conv3 = tf.keras.layers.LeakyReLU()( conv3 )
    conv3 = tf.keras.layers.Conv2D( 128 , kernel_size=( 3 , 3 ) , strides=1 )( conv3 )
    conv3 = tf.keras.layers.LeakyReLU()( conv3 )

    bottleneck = tf.keras.layers.Conv2D( 128 , kernel_size=( 3 , 3 ) , strides=1 , activation='tanh' , padding='same' )( conv3 )

    concat_1 = tf.keras.layers.Concatenate()( [ bottleneck , conv3 ] )
    conv_up_3 = tf.keras.layers.Conv2DTranspose( 128 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' )( concat_1 )
    conv_up_3 = tf.keras.layers.Conv2DTranspose( 128 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' )( conv_up_3 )
    conv_up_3 = tf.keras.layers.Conv2DTranspose( 64 , kernel_size=( 5 , 5 ) , strides=1 , activation='relu' )( conv_up_3 )

    concat_2 = tf.keras.layers.Concatenate()( [ conv_up_3 , conv2 ] )
    conv_up_2 = tf.keras.layers.Conv2DTranspose( 64 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' )( concat_2 )
    conv_up_2 = tf.keras.layers.Conv2DTranspose( 64 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' )( conv_up_2 )
    conv_up_2 = tf.keras.layers.Conv2DTranspose( 32 , kernel_size=( 5 , 5 ) , strides=1 , activation='relu' )( conv_up_2 )

    concat_3 = tf.keras.layers.Concatenate()( [ conv_up_2 , conv1 ] )
    conv_up_1 = tf.keras.layers.Conv2DTranspose( 32 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu')( concat_3 )
    conv_up_1 = tf.keras.layers.Conv2DTranspose( 32 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu')( conv_up_1 )
    conv_up_1 = tf.keras.layers.Conv2DTranspose( 3 , kernel_size=( 5 , 5 ) , strides=1 , activation='relu')( conv_up_1 )

    model = tf.keras.models.Model( inputs , conv_up_1 )

    return model

"""
### **B. Discriminator**

The discriminator model, represented as $D$, will take in the *real image* $y$ ( from the training data ) and the *generated image* $G(x)$ ( from the generator ) to output two probabilities.

* We train the discriminator in such a manner that is able to differentiate the *real images* and the generated *images*. So, we train the model such that $y$ produces a output of $1.0$ and $G(x)$ produces an output of $0.0$.
* Note that instead of using hard labels like $1.0$ and $0.0$, we use soft labels which are close to 1 and 0. So for a hard label of $1.0$, the soft label will be $(1 - \epsilon)$ where $\epsilon$ is picked uniformly from $( 0 , 0.1 ]$
"""

def get_discriminator_model():
    layers = [
        tf.keras.layers.Conv2D( 32 , kernel_size=( 7 , 7 ) , strides=1 , activation='relu' , input_shape=( img_size , img_size , 3 ) ),
        tf.keras.layers.Conv2D( 32 , kernel_size=( 7, 7 ) , strides=1, activation='relu'  ),
        tf.keras.layers.MaxPooling2D(),
        tf.keras.layers.Conv2D( 64 , kernel_size=( 5 , 5 ) , strides=1, activation='relu'  ),
        tf.keras.layers.Conv2D( 64 , kernel_size=( 5 , 5 ) , strides=1, activation='relu'  ),
        tf.keras.layers.MaxPooling2D(),
        tf.keras.layers.Conv2D( 128 , kernel_size=( 3 , 3 ) , strides=1, activation='relu'  ),
        tf.keras.layers.Conv2D( 128 , kernel_size=( 3 , 3 ) , strides=1, activation='relu'  ),
        tf.keras.layers.MaxPooling2D(),
        tf.keras.layers.Conv2D( 256 , kernel_size=( 3 , 3 ) , strides=1, activation='relu'  ),
        tf.keras.layers.Conv2D( 256 , kernel_size=( 3 , 3 ) , strides=1, activation='relu'  ),
        tf.keras.layers.MaxPooling2D(),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense( 512, activation='relu'  )  ,
        tf.keras.layers.Dense( 128 , activation='relu' ) ,
        tf.keras.layers.Dense( 16 , activation='relu' ) ,
        tf.keras.layers.Dense( 1 , activation='sigmoid' )
    ]
    model = tf.keras.models.Sequential( layers )
    return model

"""
### **C. Loss Functions**

We'll now implement the loss functions for our GAN model. As you might know that we have two loss functions, one for the generator and another for the discriminator.

* For our generator, we'll use the L2/MSE loss function.
* For optimization, we use the Adam optimizer with a learning rate of 0.0005

"""

cross_entropy = tf.keras.losses.BinaryCrossentropy()
mse = tf.keras.losses.MeanSquaredError()

def discriminator_loss(real_output, fake_output):
    real_loss = cross_entropy(tf.ones_like(real_output) - tf.random.uniform( shape=real_output.shape , maxval=0.1 ) , real_output)
    fake_loss = cross_entropy(tf.zeros_like(fake_output) + tf.random.uniform( shape=fake_output.shape , maxval=0.1  ) , fake_output)
    total_loss = real_loss + fake_loss
    return total_loss

def generator_loss(fake_output , real_y):
    real_y = tf.cast( real_y , 'float32' )
    return mse( fake_output , real_y )

generator_optimizer = tf.keras.optimizers.Adam( 0.0005 )
discriminator_optimizer = tf.keras.optimizers.Adam( 0.0005 )

generator = get_generator_model()
discriminator = get_discriminator_model()

"""
## **3. Training The GAN**

So finally, we'll train our GAN on the dataset, we prepared earlier.
"""

@tf.function
def train_step( input_x , real_y ):

    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
        # Generate an image -> G( x )
        generated_images = generator( input_x , training=True)
        # Probability that the given image is real -> D( x )
        # print(real_y)
        real_output = discriminator( real_y, training=True)
        # Probability that the given image is the one generated -> D( G( x ) )
        generated_output = discriminator(generated_images, training=True)

        # L2 Loss -> || y - G(x) ||^2
        gen_loss = generator_loss( generated_images , real_y )
        # Log loss for the discriminator
        disc_loss = discriminator_loss( real_output, generated_output )

        losses["D"].append(disc_loss.numpy())
        losses["G"].append(gen_loss.numpy())
    #tf.keras.backend.print_tensor( tf.keras.backend.mean( gen_loss ) )
    #tf.keras.backend.print_tensor( gen_loss + disc_loss )

    # Compute the gradients
    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)
    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)

    # Optimize with Adam
    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))
    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))


generator.compile(
    optimizer=generator_optimizer,
    loss=generator_loss,
    metrics=['accuracy']
)

discriminator.compile(
    optimizer=discriminator_optimizer,
    loss=discriminator_loss,
    metrics=['accuracy']
)

"""## Visualising the Generator model"""

plot_model(generator, to_file='generator.png', show_shapes=True, show_layer_names=True, dpi=66)

plot_model(discriminator, to_file='discriminator.png', show_shapes=True, show_layer_names=True, dpi=66)

"""

Run the cell below, to start the training"""

gpu_devices = tf.config.list_physical_devices('GPU')
print(gpu_devices)

if gpu_devices:
    # Choose the first GPU
    tf.config.experimental.set_visible_devices(gpu_devices[0], 'GPU')

def plot_loss(losses):
    """
    @losses.keys():
        0: loss
        1: accuracy
    """
    g_loss = []
    d_loss = []

    count = 0
    for i in losses['D']:
      count += 1
      if(count == 36):
        d_loss.append(i)
        count = 0

    count = 0
    for i in losses['G']:
      count += 1
      if(count == 36):
        g_loss.append(i)
        count = 0

    plt.figure(figsize=(10,8))
    plt.plot(d_loss, label="Discriminator loss")
    plt.plot(g_loss, label="Generator loss")
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# Please have a look at the Notebook in pdf form that was train on 150 epoc.
# num_epochs = 100
# losses = {"D":[], "G":[]}
# for e in range( num_epochs ):
#     print("Running epoch : ", e )
#     print("Running losses : ", losses["D"], losses["G"][-1] )
#     # dataset = tf.data.Dataset.from_tensor_slices( ( x_train , y_train ) ).shuffle( 1000 ).batch( 128 )
#     # dataset
#     for ( x , y ) in dataset:
#         # Here ( x , y ) represents a batch from our training dataset.
#         # print( y.shape )
#         train_step( x , y )
#     generator.save('/content/drive/MyDrive/cv_dat/gen')
#     discriminator.save('/content/drive/MyDrive/cv_dat/dis')

num_epochs = 100
losses = {"D": [], "G": []}
best_losses = {"D": float('inf'), "G": float('inf')}  # Initialize with a high value

for e in range(num_epochs):
    print("Running epoch:", e)
    print("Running losses:", losses["D"], losses["G"])

    for (x, y) in dataset:
        train_step(x, y)

    # Check if current losses are better than best losses
    if losses["D"][-1] < best_losses["D"] or losses["G"][-1] < best_losses["G"]:
        # Update best losses
        best_losses["D"] = losses["D"][-1]
        best_losses["G"] = losses["G"][-1]

        # Save the models
        generator.save('/content/drive/MyDrive/cv_dat/gen_best')
        discriminator.save('/content/drive/MyDrive/cv_dat/dis_best')

# # Final models saved after training is complete
# generator.save('/content/drive/MyDrive/cv_dat/gen_final')
# discriminator.save('/content/drive/MyDrive/cv_dat/dis_final')

"""
## **4. Results**

We plotted the input, output and the original images respectively, from a part of the dataset to find out the results.
"""

xx = []
for ( x , y ) in dataset:
        # Here ( x , y ) represents a batch from our training dataset.
        # print( x.shape )
        xx.append( x )

print(len(xx))
# print(xx[-1])
# for i in xx[-1]:
#   print(i.shape)
y=generator( xx[0] ).numpy()
print(y.shape)
plt.subplot(2,2,1)
plt.imshow(y[0])
plt.subplot(2,2,2)

plt.imshow(x[0])

# y=generator( test_x ).numpy()
y = generator( test_x[0 : ] ).numpy()

y = generator( train_x[2000 : 2100] ).numpy()

target_size =( 350 , 350 )
for i in range(2000, 2100, 1):
  plt.figure(figsize=(25,25))
  or_image = plt.subplot(3,3,1)
  or_image.set_title('Grayscale Input', fontsize=16)
  #plt.imshow( train_x[i].reshape((120,120)) , cmap='gray' )
  resized_img = cv2.resize(train_x[i],target_size , interpolation=cv2.INTER_LINEAR)
  plt.imshow( resized_img, cmap='gray' )

  in_image = plt.subplot(3,3,2)
  image = Image.fromarray( ( y[i-2000] * 255 ).astype( 'uint8' )  )
  image = np.asarray( image )
  in_image.set_title('Colorized Output', fontsize=16)
  # plt.imshow( image )
  resized_img = cv2.resize(image, target_size , interpolation=cv2.INTER_LINEAR)
  plt.imshow( resized_img )


  ou_image = plt.subplot(3,3,3)
  image = Image.fromarray( ( train_y[i] * 255 ).astype( 'uint8' ) ).resize( ( 350 , 350 ) )
  ou_image.set_title('Ground Truth', fontsize=16)
  plt.imshow( image )

  plt.show()

for i in range(1, 100, 5):
  plt.figure(figsize=(25,25))
  or_image = plt.subplot(3,3,1)
  or_image.set_title('Grayscale Input', fontsize=16)
  plt.imshow( test_x[i].reshape((120,120)) , cmap='gray' )

  in_image = plt.subplot(3,3,2)
  image = Image.fromarray( ( y[i] * 255 ).astype( 'uint8' ) ).resize( ( 360 , 360 ) )
  image = np.asarray( image )
  in_image.set_title('Colorized Output', fontsize=16)
  plt.imshow( image )

  ou_image = plt.subplot(3,3,3)
  image = Image.fromarray( ( test_y[i] * 255 ).astype( 'uint8' ) ).resize( ( 360 , 360 ) )
  ou_image.set_title('Ground Truth', fontsize=16)
  plt.imshow( image )

  plt.show()

"""Therefore, the overwhelming results depicts the power of GANs and the disruption which can be broght through them."""